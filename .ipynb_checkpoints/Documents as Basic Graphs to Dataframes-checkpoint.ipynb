{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a2edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbugueno/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mbugueno/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import dgl\n",
    "import itertools\n",
    "import numpy as np\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random,re, os, nltk\n",
    "import sys\n",
    "import tqdm\n",
    "import torch\n",
    "import time, datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import word2vec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from graph_utils import *\n",
    "from generation_module import *\n",
    "from itertools import compress\n",
    "from loader_TextLevel import dataset_to_dataframe\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc501e",
   "metadata": {},
   "source": [
    "## Load data\n",
    "### App Reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ce9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(path_to_save, dataset, df, num=7000, n_short=3, mode=\"training\", preprocess=\"soft\"):\n",
    "    #path_to_save == RBased path\n",
    "    \n",
    "    labels = [str(lab) for lab in df['label'][:num]] \n",
    "    if preprocess==\"soft\":\n",
    "        corpus = [clean_str(doc, preprocess) for doc in df[\"content\"][:num]] \n",
    "        \n",
    "    if preprocess==\"tlgcn\":\n",
    "        corpus = [remove_short(doc, n_short) for doc in corpus]  \n",
    "    \n",
    "    if mode!=\"test\":\n",
    "        print (\"Processing training set...\")\n",
    "    else:\n",
    "        print (\"Processing testing set...\")\n",
    "    \n",
    "    tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "    results = []\n",
    "    for doc in tokenized_corpus:\n",
    "        results.append(' '.join([word for word in doc]))\n",
    "    \n",
    "    valid_mask=[]\n",
    "    for doc in results:\n",
    "        if doc!='':\n",
    "            valid_mask.append(True)\n",
    "        else:\n",
    "            valid_mask.append(False)\n",
    "    \n",
    "    final_results=list(compress(results, valid_mask))\n",
    "    final_labels = list(compress(labels, valid_mask))\n",
    "    \n",
    "    print (len(final_results), \"/\",num, \"samples have been generated.\")    \n",
    "    \n",
    "    results = list(zip(final_labels, final_results))\n",
    "    \n",
    "    if mode!=\"test\":\n",
    "        df = pd.DataFrame(results, columns = [\"label\", \"content\"])\n",
    "        df.to_csv(path_to_save+dataset+\"/\"+preprocess+\"/source_processed.csv\",index=False)\n",
    "        \n",
    "    else:    \n",
    "        df = pd.DataFrame(results, columns = [\"label\", \"content\"])\n",
    "        df.to_csv(path_to_save+dataset+\"/\"+preprocess+\"/source_processed_test.csv\",index=False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e15412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 3.12kB [00:00, 825kB/s]                                                                                                                       \n",
      "Downloading metadata: 1.68kB [00:00, 569kB/s]                                                                                                                             \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset app_reviews/default (download: 40.62 MiB, generated: 31.25 MiB, post-processed: Unknown size, total: 71.87 MiB) to /home/mbugueno/.cache/huggingface/datasets/app_reviews/default/0.0.0/20335b51b604b9bc04b7be253cd8445caa9ba93f15f39a4b0492b9e9102853de...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 42.6MB [00:00, 88.4MB/s]                                                                                                                                \n",
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset app_reviews downloaded and prepared to /home/mbugueno/.cache/huggingface/datasets/app_reviews/default/0.0.0/20335b51b604b9bc04b7be253cd8445caa9ba93f15f39a4b0492b9e9102853de. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 310.69it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = dataset_to_dataframe(\"app_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af325eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "6971 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "2992 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "6859 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "2950 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus(path_to_save, \"app_reviews\", df_train, mode=\"training\", num=7000, preprocess=\"soft\")\n",
    "clean_corpus(path_to_save, \"app_reviews\", df_test, mode=\"test\", num=3000, preprocess=\"soft\")\n",
    "\n",
    "clean_corpus(path_to_save, \"app_reviews\", df_train, mode=\"training\", num=7000, preprocess=\"tlgcn\")\n",
    "clean_corpus(path_to_save, \"app_reviews\", df_test, mode=\"test\", num=3000, preprocess=\"tlgcn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b53f09",
   "metadata": {},
   "source": [
    "### App Reviews\n",
    "\n",
    "#### Graphs to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24b5a558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonder ful fine'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess= \"soft\"\n",
    "path_to_save = \"../RBased\"\n",
    "path_read = \"../DataGraphs\"\n",
    "df_train=pd.read_csv(path_to_save+\"app_reviews/\"+preprocess+\"/source_processed.csv\")\n",
    "df_test=pd.read_csv(path_to_save+\"app_reviews/\"+preprocess+\"/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8fe321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.169559604074021 \tMax: 72\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.186497326203209 \tMax: 67\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.169559604074021 \tMax: 72\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.186497326203209 \tMax: 67\n"
     ]
    }
   ],
   "source": [
    "#1.A -- Window\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_app_prep/1.A/raw/App_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_app_prep/1.A/raw/App_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2 -- Window extended\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_app_prep/1.A2/raw/App_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_app_prep/1.A2/raw/App_graphs_A2_test.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e56f8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.169559604074021 \tMax: 72\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.186497326203209 \tMax: 67\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.169559604074021 \tMax: 72\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.186497326203209 \tMax: 67\n"
     ]
    }
   ],
   "source": [
    "#1.C -- Sequence\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_app_prep/1.C/raw/App_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_app_prep/1.C/raw/App_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2 -- Sequence simplified\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_app_prep/1.C2/raw/App_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_app_prep/1.C2/raw/App_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990f0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_corpus(datasets, path_to_save):\n",
    "    for dataset in datasets:\n",
    "        print (\"\\n--------------------------\\nRunning data split for\", dataset)\n",
    "        df_train, df_test = dataset_to_dataframe(dataset)\n",
    "\n",
    "        clean_corpus(path_to_save, dataset, df_train, mode=\"training\", num=7000, preprocess=\"soft\")\n",
    "        clean_corpus(path_to_save, dataset, df_test, mode=\"test\", num=3000, preprocess=\"soft\")\n",
    "\n",
    "        #clean_corpus(path_to_save, dataset, df_train, mode=\"training\", num=7000, preprocess=\"tlgcn\")\n",
    "        #clean_corpus(path_to_save, dataset, df_test, mode=\"test\", num=3000, preprocess=\"tlgcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f045f73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "Running data split for imdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/mbugueno/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 236.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "\n",
      "--------------------------\n",
      "Running data split for dbpedia_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.22kB [00:00, 867kB/s]                                                                                                                       \n",
      "Downloading metadata: 2.47kB [00:00, 592kB/s]                                                                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /home/mbugueno/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68.3M/68.3M [00:03<00:00, 19.7MB/s]\n",
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dbpedia_14 downloaded and prepared to /home/mbugueno/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 438.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "\n",
      "--------------------------\n",
      "Running data split for hyperpartisan_news_detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.96kB [00:00, 797kB/s]                                                                                                                       \n",
      "Downloading metadata: 5.04kB [00:00, 619kB/s]                                                                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset hyperpartisan_news_detection/byarticle (download: 976.91 KiB, generated: 2.67 MiB, post-processed: Unknown size, total: 3.63 MiB) to /home/mbugueno/.cache/huggingface/datasets/hyperpartisan_news_detection/byarticle/1.0.0/7f4215b0474950ddf516e806400ab81d098b3da3b3a919a13cd1a4cf2c677012...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                                                                        | 0.00/972k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 972k/972k [00:00<00:00, 5.88MB/s]\u001b[A\n",
      "Downloading data files:  50%|███████████████████████████████████████████████████████▌                                                       | 1/2 [00:01<00:01,  1.45s/it]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.5k/28.5k [00:00<00:00, 6.54MB/s]\u001b[A\n",
      "Downloading data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.27s/it]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 60.52it/s]\n",
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hyperpartisan_news_detection downloaded and prepared to /home/mbugueno/.cache/huggingface/datasets/hyperpartisan_news_detection/byarticle/1.0.0/7f4215b0474950ddf516e806400ab81d098b3da3b3a919a13cd1a4cf2c677012. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 558.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "516 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "129 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "516 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "129 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "516 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "129 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "516 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "129 / 3000 samples have been generated.\n",
      "\n",
      "--------------------------\n",
      "Running data split for bbc\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data_Graphs/BBC/Raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m datasets\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbpedia_14\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperpartisan_news_detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclean_all_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mclean_all_corpus\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning data split for\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset)\n\u001b[0;32m----> 7\u001b[0m     df_train, df_test \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     clean_corpus(dataset, df_train, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7000\u001b[39m, preprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     clean_corpus(dataset, df_test, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, preprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/graph-based_TR/loader_TextLevel.py:116\u001b[0m, in \u001b[0;36mdataset_to_dataframe\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    113\u001b[0m         df_temp\u001b[38;5;241m=\u001b[39mfrom_dataset2df(dataset, my_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], columns)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#para bbc manually set dataframe \u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     df_temp\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Data_Graphs/BBC/Raw.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     df_temp\u001b[38;5;241m=\u001b[39mdf_temp\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m#read from path and split\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data_Graphs/BBC/Raw.csv'"
     ]
    }
   ],
   "source": [
    "datasets=[\"imdb\", \"dbpedia_14\", \"hyperpartisan_news_detection\", \"bbc\"]\n",
    "clean_all_corpus(datasets, path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ca685",
   "metadata": {},
   "source": [
    "### DBpedia_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d3ac9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naucleopsis oblongifolia species plant moraceae family endemic brazil'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(path_to_save+\"dbpedia_14/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(path_to_save+\"dbpedia_14/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06ea60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 2 \tAverage: 26.325714285714287 \tMax: 124\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 26.165 \tMax: 101\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 2 \tAverage: 26.325714285714287 \tMax: 124\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 26.165 \tMax: 101\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 2 \tAverage: 26.325714285714287 \tMax: 124\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 26.165 \tMax: 101\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 2 \tAverage: 26.325714285714287 \tMax: 124\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 26.165 \tMax: 101\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_db_prep/1.A/raw/DB_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_db_prep/1.A/raw/DB_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_db_prep/1.A2/raw/DB_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_db_prep/1.A2/raw/DB_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_db_prep/1.C/raw/DB_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_db_prep/1.C/raw/DB_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_db_prep/1.C2/raw/DB_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_db_prep/1.C2/raw/DB_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4330406",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9295a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'know anything film based best selling book tried guessing opening scenes would best guess buried treasure death list lasted 15 minutes got sneaky suspicion film crap bore bad plot acting anyone gave two stars must work film makers watched hero jumped corvette rush hospital suit shirt tie arrived hospital jeans shirt even get continuity right got christian theme hard miss'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(path_to_save+\"imdb/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(path_to_save+\"imdb/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "451f2a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 100.99114285714286 \tMax: 551\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 97.87833333333333 \tMax: 518\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 100.99114285714286 \tMax: 551\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 97.87833333333333 \tMax: 518\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 100.99114285714286 \tMax: 551\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 97.87833333333333 \tMax: 518\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 100.99114285714286 \tMax: 551\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 97.87833333333333 \tMax: 518\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_imdb_prep/1.A/raw/IMDB_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_imdb_prep/1.A/raw/IMDB_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_imdb_prep/1.A2/raw/IMDB_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_imdb_prep/1.A2/raw/IMDB_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_imdb_prep/1.C/raw/IMDB_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_imdb_prep/1.C/raw/IMDB_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_imdb_prep/1.C2/raw/IMDB_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_imdb_prep/1.C2/raw/IMDB_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ac2cf",
   "metadata": {},
   "source": [
    "### hyperpartisan_news_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bbc17d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p charlottesville va reporter botched rolling stone article brutal gang rape university virginia willfully ignored facts statements disproved preconceived storyline school 8217 callousness toward sexual assault victims attorney former associate dean students said closing arguments tuesday p p university administrator nicole eramo seeking 7 5 million magazine portrayal 2014 story sabrina rubin erdely alleged sexual assault woman identified 8220 jackie 8221 eramo claims unfairly portrayed article trying sweep jackie 8217 sexual assault rug order protect university p p attorney tom clare argued tuesday erdely set beginning tell story 8220 institutional indifference 8221 brushed statements sources 8217 fit narrative pushed views administration onto vulnerable women interviewing p p 8220 decided article going 8217 matter facts 8221 clare said p p story jackie 8217 rape set firestorm university virginia schools nationwide prompted police launch investigation alleged assault eramo received hundreds angry letters emails faced protesters outside office story crumbled news outlets began asking questions police found evidence back jackie 8217 claims article officially retracted april 2015 p p eramo must prove rolling stone statements made appear 8220 odious infamous ridiculous 8221 magazine acted 8220 actual malice 8221 meaning knew writing false known false p p lawyer rolling stone scott sexton said closing statements evidence magazine knew botched story gang rape university virginia false publishing p p women erdely interviewed including jackie told eramo fiercest advocate clare argued erdely invested preconceived storyline 8220 blind facts 8221 argued erdely purposely set make eramo 8220 villain 8221 knew 8220 easy target 8221 8217 speak publicly jackie 8217 case due federal privacy laws p p 8220 8217 reckless 8217 cavalier 8217 intentional 8221 clare said p p course two week trial 10 jurors watched 11 hours video testimony heard dozen live witnesses examined nearly 300 exhibits seven jurors deliberate three named alternates p p 169 2016 associated press rights reserved material may published broadcast rewritten redistributed p'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(path_to_save+\"hyperpartisan_news_detection/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(path_to_save+\"hyperpartisan_news_detection/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac1e64bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 230.51162790697674 \tMax: 1240\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 28 \tAverage: 234.50387596899225 \tMax: 824\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 230.51162790697674 \tMax: 1240\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 28 \tAverage: 234.50387596899225 \tMax: 824\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 230.51162790697674 \tMax: 1240\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 28 \tAverage: 234.50387596899225 \tMax: 824\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 230.51162790697674 \tMax: 1240\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 28 \tAverage: 234.50387596899225 \tMax: 824\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_hyperpartisan_news_detection_prep/1.A/raw/Hyper_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_hyperpartisan_news_detection_prep/1.A/raw/Hyper_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_hyperpartisan_news_detection_prep/1.A2/raw/Hyper_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_hyperpartisan_news_detection_prep/1.A2/raw/Hyper_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_hyperpartisan_news_detection_prep/1.C/raw/Hyper_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_hyperpartisan_news_detection_prep/1.C/raw/Hyper_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_hyperpartisan_news_detection_prep/1.C2/raw/Hyper_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_hyperpartisan_news_detection_prep/1.C2/raw/Hyper_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781960bc",
   "metadata": {},
   "source": [
    "### BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "850187de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'legendary music studio close new york music studio john lennon spent final hours close next month hit factory opened 37 years ago played host biggest stars music including paul simon madonna david bowie however rise digital recording affected business studio relocating smaller premises miami lennon recorded final album double fantasy studio 1979 studio founded jerry ragovoy 1968 1975 sold fellow producer edward germano turned 100 000 square foot studio seven recording rooms live mastering suites first client stevie wonder recorded part classic album songs key life well known albums recorded partially recorded studio include bruce springsteen born usa rolling stones emotional rescue paul simon graceland michael jackson billy joel jay z beyonce also among artists used hit factory past well 50 cent survived attempt life left premises 2000 studio made history 1994 record 41 grammy nominations songs recorded mastered mixed including soundtrack whitney houston film bodyguard'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(path_to_save+\"bbc/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(path_to_save+\"bbc/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfe765d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 42 \tAverage: 158.19044943820225 \tMax: 1149\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 64 \tAverage: 165.59101123595505 \tMax: 819\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 42 \tAverage: 158.19044943820225 \tMax: 1149\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 64 \tAverage: 165.59101123595505 \tMax: 819\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 42 \tAverage: 158.19044943820225 \tMax: 1149\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 64 \tAverage: 165.59101123595505 \tMax: 819\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 42 \tAverage: 158.19044943820225 \tMax: 1149\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 64 \tAverage: 165.59101123595505 \tMax: 819\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_bbc_prep/1.A/raw/BBC_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(path_read+\"data_bbc_prep/1.A/raw/BBC_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_bbc_prep/1.A2/raw/BBC_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(path_read+\"data_bbc_prep/1.A2/raw/BBC_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_bbc_prep/1.C/raw/BBC_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(path_read+\"data_bbc_prep/1.C/raw/BBC_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_bbc_prep/1.C2/raw/BBC_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(path_read+\"data_bbc_prep/1.C2/raw/BBC_graphs_C2_test.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
