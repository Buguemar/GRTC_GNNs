{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087c4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a2edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbugueno/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mbugueno/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "from loader_TextLevel import dataset_to_dataframe\n",
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import dgl\n",
    "import torch.nn.functional as F\n",
    "import word2vec\n",
    "import itertools\n",
    "import torch\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "import sys, random\n",
    "import argparse\n",
    "from itertools import compress\n",
    "import time, datetime\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random,re, os, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from itertools import compress\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7a5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_utils import *\n",
    "from generation_module import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cc501e",
   "metadata": {},
   "source": [
    "## Load data\n",
    "### App Reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ea3b3",
   "metadata": {},
   "source": [
    "\n",
    "def num_co(a,b,lista, mode='forward'):\n",
    "    #mode forward co-occurrence from left to right\"\n",
    "    count=0\n",
    "    for i in range(len(lista)-1):\n",
    "        if lista[i]==a and lista[i+1]==b:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def set_nodes(list_text):\n",
    "    nodes=set(list_text)\n",
    "    return nodes\n",
    "\n",
    "def set_edges(list_text, mode='forward', ws=1, weighted=True, directed=True): \n",
    "    #mode: \n",
    "    #forward, from left to right considering a window size=1 1.C and 1.C2\n",
    "    #window, given a term A, it defines outgoing edges to the surrounding terms B\n",
    "    #source -- target (undirected)\n",
    "    #source -> target (directed)\n",
    "    edges=[]\n",
    "    if mode=='forward': #same for directed or not\n",
    "        for i in range(len(list_text)-1):\n",
    "            if directed:\n",
    "                if weighted:\n",
    "                    times=num_co(list_text[i],list_text[i+1],list_text)\n",
    "                    if (list_text[i],list_text[i+1], times) not in edges:\n",
    "                        edges.append((list_text[i],list_text[i+1], times))\n",
    "                else:\n",
    "                    edges.append((list_text[i],list_text[i+1]))\n",
    "            else:\n",
    "                if weighted:\n",
    "                    timesa=num_co(list_text[i],list_text[i+1],list_text)\n",
    "                    timesb=num_co(list_text[i+1],list_text[i],list_text)\n",
    "                    tuplar=(list_text[i+1],list_text[i],timesa+timesb)\n",
    "                    if tuplar not in edges:\n",
    "                        edges.append((list_text[i],list_text[i+1],timesa+timesb))\n",
    "                        #nuevo\n",
    "                        edges.append((list_text[i+1],list_text[i],timesa+timesb))\n",
    "                else:\n",
    "                    tuplar=(list_text[i+1],list_text[i])\n",
    "                    if tuplar not in edges:\n",
    "                        edges.append((list_text[i],list_text[i+1]))\n",
    "                        # nuevo\n",
    "                        edges.append((list_text[i+1],list_text[i]))\n",
    "    \n",
    "    elif mode=='window':\n",
    "        dict_edges={}\n",
    "        #forward\n",
    "        for i in range(len(list_text)):\n",
    "            for w in range(1,ws+1):\n",
    "                if directed:\n",
    "                    try:\n",
    "                        if (list_text[i],list_text[i+w]) not in dict_edges.keys():\n",
    "                            dict_edges[(list_text[i],list_text[i+w])]=1\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    try:\n",
    "                        if (list_text[i],list_text[i+w]) not in dict_edges.keys() and (list_text[i+w],list_text[i]) not in dict_edges.keys():\n",
    "                            dict_edges[(list_text[i],list_text[i+w])]=1\n",
    "                        elif (list_text[i],list_text[i+w]) in dict_edges.keys():\n",
    "                            dict_edges[(list_text[i],list_text[i+w])]+=1\n",
    "                        elif (list_text[i+w],list_text[i]) in dict_edges.keys():\n",
    "                            dict_edges[(list_text[i+w],list_text[i])]+=1\n",
    "                    except:\n",
    "                        continue\n",
    " \n",
    "        #backward\n",
    "        list_text.reverse()\n",
    "        for i in range(len(list_text)):\n",
    "            for w in range(1,ws+1):\n",
    "                if directed:\n",
    "                    try:\n",
    "                        if (list_text[i],list_text[i+w]) not in dict_edges.keys():\n",
    "                            dict_edges[(list_text[i],list_text[i+w])]=1\n",
    "                        else: \n",
    "                            dict_edges[(list_text[i],list_text[i+w])]+=1\n",
    "                    except:\n",
    "                        continue\n",
    "                      \n",
    "        #once the dictionary is created        \n",
    "        if weighted:                  \n",
    "            edges=[(tupla[0],tupla[1],dict_edges[tupla]) for tupla in dict_edges.keys()]\n",
    "            # nuevo\n",
    "            invertir=[(tupla[1],tupla[0],dict_edges[tupla]) for tupla in dict_edges.keys()]\n",
    "        else:\n",
    "            edges=list(dict_edges.keys())\n",
    "            # nuevo \n",
    "            invertir=[(tupla[1],tupla[0]) for tupla in dict_edges.keys()]\n",
    "        #nuevo    \n",
    "        edges+=invertir\n",
    "        \n",
    "    return edges\n",
    "\n",
    "def tokenize_sentences(data, labels, tokenizer=\"nltk\"): \n",
    "    #obtain sentences \n",
    "    #total=len(data)\n",
    "    #ids_q=[np.random.randint(total) for i in range(num_graphs)]\n",
    "    labels_return= np.asarray(labels)\n",
    "    #sents_data = [sent_tokenize(data[ide]) for ide in ids_q]\n",
    "    #lens_sent = [len(x) for x in sents_data]\n",
    "    docs=np.asarray(data)\n",
    "    #print (\"\\nLenght of sampled text (w.r.t. #sentences):\")\n",
    "    #print (\"Min:\", np.min(sents), \"\\tAverage:\", np.mean(sents),\"\\tMax:\",np.max(sents))\n",
    "    #load tokenizer para sentencias\n",
    "    #tokenized=[]\n",
    "    \n",
    "    if tokenizer==\"bert\":\n",
    "        mytokenizer = AutoTokenizer.from_pretrained('bert-base-cased')    \n",
    "        # Parallel(n_jobs=-1)(delayed(clean_text)(sentence) for sentence in sample)\n",
    "        temp_tokenized = [mytokenizer(sample) for sample in docs]\n",
    "        tokens=[np.asarray(text['input_ids']) for text in temp_tokenized]\n",
    "        print (\"BERT tokens\", tokens)\n",
    "        #tokens=[np.concatenate(np.asarray(text['input_ids'])) for text in temp_tokenized]\n",
    "        lens = [len(set(x)) for x in tokens]  ##3lista con ntokens diferentes por review\n",
    "        all_tokens=np.concatenate([list(set(x)) for x in tokens]) #tenia concatenate \n",
    "        for temp in temp_tokenized:\n",
    "            tokenized.append([mytokenizer.convert_ids_to_tokens(doc[1:-1]) for doc in temp['input_ids']])\n",
    "        \n",
    "    else:\n",
    "        all_tokens=[]\n",
    "        #for sample in sents_data:\n",
    "        # Parallel(n_jobs=-1)(delayed(clean_text)(sentence) for sentence in sample)\n",
    "        temp_tokenized = [word_tokenize(sample) for sample in docs]\n",
    "        #tokenizer = [wordpunct_tokenize(sample) for sample in data]\n",
    "        #tokenized.append(temp_tokenized)\n",
    "        lens = [len(set(sample)) for sample in temp_tokenized]  \n",
    "        tokens=np.concatenate([list(set(sample)) for sample in temp_tokenized])\n",
    "        for tokens_review in tokens:\n",
    "            all_tokens+=set(tokens_review)\n",
    "\n",
    "    lens=np.asarray(lens)        \n",
    "    print (\"Vocabulary size (\",tokenizer,\"):\", len(set(all_tokens)))\n",
    "    print (\"\\nLenght of text (w.r.t. #tokens):\")\n",
    "    print (\"Min:\", np.min(lens), \"\\tAverage:\", np.mean(lens),\"\\tMax:\",np.max(lens))\n",
    "    \n",
    "    return temp_tokenized, labels_return #era tokenized, labels_return\n",
    "\n",
    "def check(tupla,all_edges): ###entrega True si existe la tupla en edges\n",
    "    for tup in all_edges:\n",
    "        if tup[0]==tupla[0] and tup[1]==tupla[1]:\n",
    "            return True, tup[-1]\n",
    "    return False, None\n",
    "    \n",
    "def update(tupla, cont, all_edges):##re\n",
    "    all_edges.remove((tupla[0],tupla[1],cont))\n",
    "    all_edges.append((tupla[0],tupla[1],cont+1))\n",
    "    return all_edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e66a0801",
   "metadata": {},
   "source": [
    "def create_graphs(data_in, labels_in, weighted=False, directed=False, mode=\"forward\", ws=1, plot=False):\n",
    "    #samples es lista de muestras tokenizadas por sentencia\n",
    "    #crear ficheros para grafos training set -- nodos por grafo, nodos y sus features, labels de cada grafo, edges\n",
    "    #\n",
    "    samples, labels = tokenize_sentences(data_in, labels_in)\n",
    "    \n",
    "    ngraph=0\n",
    "    review2labels=[]\n",
    "    review2nodes=[]\n",
    "    review2edges=[]\n",
    "    total=len(samples)\n",
    "    for doc in samples:        \n",
    "        nodes=set_nodes(doc)\n",
    "        edges=set_edges(doc, weighted=weighted, mode=mode, ws=ws, directed=directed)\n",
    "        \n",
    "        all_nodes=set(nodes)\n",
    "        all_edges=set(edges)\n",
    "        \n",
    "        review2nodes.append(all_nodes)\n",
    "        review2edges.append(all_edges)\n",
    "        review2labels.append(labels[ngraph])\n",
    "        ngraph+=1\n",
    "        \n",
    "        ### codigo para graficar el grafo completo del review \n",
    "        if plot:\n",
    "            plt.figure(figsize=(10,10))\n",
    "            if directed:\n",
    "                G = nx.DiGraph()\n",
    "            else:\n",
    "                G = nx.Graph()\n",
    "            G.add_nodes_from(all_nodes)\n",
    "            pos=nx.spring_layout(G)\n",
    "            if weighted:\n",
    "                G.add_weighted_edges_from(all_edges)\n",
    "                edge_labels=dict([((u,v,),d['weight']) for u,v,d in G.edges(data=True)])\n",
    "                nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)\n",
    "            else: \n",
    "                G.add_edges_from(all_edges)\n",
    "\n",
    "            nx.draw(G,pos, with_labels=True, font_weight='bold', node_size=400, node_color='cyan')\n",
    "            plt.show()\n",
    "            \n",
    "    return review2nodes, review2edges, review2labels\n",
    "\n",
    "def create_graph2Dataframe(df_text, df_labels, directed=True, plot=False, mode='forward', ws=1, weighted=False):\n",
    "    #forward con directed false == window graph con ws=1 === 1.a baseline\n",
    "    nodes,edges,labels= create_graphs(df_text, df_labels, directed=directed, plot=plot, mode=mode, ws=ws, weighted=weighted)\n",
    "    total_nodes_file=0\n",
    "    cont=0\n",
    "    sample=0\n",
    "    dict_nodes_file={}\n",
    "\n",
    "    for node_list in nodes:\n",
    "        total_nodes_file+=len(node_list)\n",
    "        for node in node_list:\n",
    "            dict_nodes_file[node+'_sample:'+str(sample)]=cont\n",
    "            cont+=1\n",
    "        sample+=1\n",
    "        \n",
    "    filas=[]\n",
    "    anterior=0\n",
    "    for i in range(len(nodes)):  \n",
    "        #text_id=ids_sample[i]\n",
    "        text=df_text[i] #text_id\n",
    "        nodos_sam=list(nodes[i]) ###node features como lista de terminos, listo\n",
    "        edges_sam=list(edges[i]) ###edges en words --- transformar\n",
    "        label=labels[i] #-1 #id de clase con -1, listo --------------- CAMBIAR EN DATASETS ---------------\n",
    "\n",
    "        f_nodos=[dict_nodes_file[nodo+\"_sample:\"+str(i)] for nodo in nodos_sam] #node ids, transformado de 0-19588, listo\n",
    "        anterior=f_nodos[0]\n",
    "        f_nodos_f=[nodo-anterior for nodo in f_nodos]\n",
    "        list_source=[]\n",
    "        list_target=[]\n",
    "        list_attr=[]\n",
    "        for edge in edges_sam:\n",
    "            source=edge[0]\n",
    "            target=edge[1]\n",
    "            if weighted:\n",
    "                edge_attr=edge[-1] ###basicamente edge[2]\n",
    "                list_attr.append(edge_attr)\n",
    "            node_source=dict_nodes_file[source+'_sample:'+str(i)]\n",
    "            node_target=dict_nodes_file[target+'_sample:'+str(i)]\n",
    "            list_source.append(node_source)\n",
    "            list_target.append(node_target)                \n",
    "\n",
    "        list_source_f=[nodo-anterior for nodo in list_source]\n",
    "        list_target_f=[nodo-anterior for nodo in list_target]\n",
    "\n",
    "        f_edges=[list_source,list_target] ## este no se guarda\n",
    "        \n",
    "        f_edges_f=[list_source_f,list_target_f] #este es el real ;)\n",
    "        fila=[f_nodos_f, nodos_sam, f_edges_f,label]\n",
    "        if weighted:\n",
    "            fila=[f_nodos_f, nodos_sam, f_edges_f,list_attr,label]\n",
    "        filas.append(fila)\n",
    "    final_columns=['nodes', 'node_features', 'edges', 'label']\n",
    "    if weighted:\n",
    "        final_columns=['nodes', 'node_features', 'edges', 'edges_attr', 'label']\n",
    "        \n",
    "    df = pd.DataFrame(filas, columns = final_columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ce9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(dataset, df, num=7000, n_short=3, mode=\"training\", preprocess=\"soft\"):\n",
    "\n",
    "    labels = [str(lab) for lab in df['label'][:num]] \n",
    "    if preprocess==\"raw\":\n",
    "        corpus = [doc for doc in df[\"content\"][:num]]\n",
    "    if preprocess!=\"raw\": #preprocessing\n",
    "        corpus = [clean_str(doc, preprocess) for doc in df[\"content\"][:num]] \n",
    "        \n",
    "    if preprocess=='tlgcn':\n",
    "        corpus = [remove_short(doc, n_short) for doc in corpus] #n_short  \n",
    "    \n",
    "    if mode!=\"test\":\n",
    "        print (\"Processing training set...\")\n",
    "    else:\n",
    "        print (\"Processing testing set...\")\n",
    "    \n",
    "    \n",
    "    tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "    results = []\n",
    "    for doc in tokenized_corpus:\n",
    "        results.append(' '.join([word for word in doc]))\n",
    "    \n",
    "    valid_mask=[]\n",
    "    for doc in results:\n",
    "        if doc!='':\n",
    "            valid_mask.append(True)\n",
    "        else:\n",
    "            valid_mask.append(False)\n",
    "    \n",
    "    final_results=list(compress(results, valid_mask))\n",
    "    final_labels = list(compress(labels, valid_mask))\n",
    "    \n",
    "    print (len(final_results), \"/\",num, \"samples have been generated.\")    \n",
    "    \n",
    "    results = list(zip(final_labels, final_results))\n",
    "    \n",
    "    if mode!=\"test\":\n",
    "        df = pd.DataFrame(results, columns = [\"label\", \"content\"])\n",
    "        df.to_csv(\"../Data_RBased/\"+dataset+\"/\"+preprocess+\"/source_processed.csv\",index=False)\n",
    "        \n",
    "    else:    \n",
    "        df = pd.DataFrame(results, columns = [\"label\", \"content\"])\n",
    "        df.to_csv(\"../Data_RBased/\"+dataset+\"/\"+preprocess+\"/source_processed_test.csv\",index=False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e15412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 3.12kB [00:00, 825kB/s]                                                                                                                       \n",
      "Downloading metadata: 1.68kB [00:00, 569kB/s]                                                                                                                             \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset app_reviews/default (download: 40.62 MiB, generated: 31.25 MiB, post-processed: Unknown size, total: 71.87 MiB) to /home/mbugueno/.cache/huggingface/datasets/app_reviews/default/0.0.0/20335b51b604b9bc04b7be253cd8445caa9ba93f15f39a4b0492b9e9102853de...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 42.6MB [00:00, 88.4MB/s]                                                                                                                                \n",
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset app_reviews downloaded and prepared to /home/mbugueno/.cache/huggingface/datasets/app_reviews/default/0.0.0/20335b51b604b9bc04b7be253cd8445caa9ba93f15f39a4b0492b9e9102853de. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 310.69it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = dataset_to_dataframe(\"app_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292ea425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197527</th>\n",
       "      <td>Wonder ful Fine</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267684</th>\n",
       "      <td>About my future</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108964</th>\n",
       "      <td>Well</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7179</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250988</th>\n",
       "      <td>OK</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                content  label\n",
       "197527  Wonder ful Fine      4\n",
       "267684  About my future      4\n",
       "108964             Well      4\n",
       "7179          Excellent      4\n",
       "250988               OK      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771ccdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Wonder ful Fine', 'About my future', 'Well', 'Excellent', 'OK',\n",
       "       'Wah',\n",
       "       \"omg. just download it. do it. this app is definitely one of the best things i've come across since using android. it's a must have for all those who want to know what's in their phone  how are apps ruining it's precious free space and want to delete rubbish right away.\",\n",
       "       'Thank is very helpful', 'Way better than the rest',\n",
       "       'It is very helpful to find apps'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "np.asarray(df_train['content'])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af325eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "6971 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "2992 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "6859 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "2950 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n"
     ]
    }
   ],
   "source": [
    "clean_corpus(\"app_reviews\", df_train, mode=\"training\", num=7000, preprocess=\"raw\")\n",
    "clean_corpus(\"app_reviews\", df_test, mode=\"test\", num=3000, preprocess=\"raw\")\n",
    "\n",
    "clean_corpus(\"app_reviews\", df_train, mode=\"training\", num=7000, preprocess=\"soft\")\n",
    "clean_corpus(\"app_reviews\", df_test, mode=\"test\", num=3000, preprocess=\"soft\")\n",
    "\n",
    "clean_corpus(\"app_reviews\", df_train, mode=\"training\", num=7000, preprocess=\"tlgcn\")\n",
    "clean_corpus(\"app_reviews\", df_test, mode=\"test\", num=3000, preprocess=\"tlgcn\")\n",
    "\n",
    "clean_corpus(\"app_reviews\", df_train, mode=\"training\", num=7000, preprocess=\"custom\")\n",
    "clean_corpus(\"app_reviews\", df_test, mode=\"test\", num=3000, preprocess=\"custom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b53f09",
   "metadata": {},
   "source": [
    "### App Reviews\n",
    "\n",
    "#### Raw Graphs to Dataframe\n",
    "\n",
    "* default: raw\n",
    "* prep: preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d0d5802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 417\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 10.955 \tMax: 130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>node_features</th>\n",
       "      <th>edges</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Wonder, ful, Fine]</td>\n",
       "      <td>[[1, 0, 2, 1], [0, 1, 1, 2]]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[About, future, my]</td>\n",
       "      <td>[[0, 2, 2, 1], [2, 0, 1, 2]]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0]</td>\n",
       "      <td>[Well]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0]</td>\n",
       "      <td>[Excellent]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0]</td>\n",
       "      <td>[OK]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nodes        node_features                         edges  label\n",
       "0  [0, 1, 2]  [Wonder, ful, Fine]  [[1, 0, 2, 1], [0, 1, 1, 2]]      4\n",
       "1  [0, 1, 2]  [About, future, my]  [[0, 2, 2, 1], [2, 0, 1, 2]]      4\n",
       "2        [0]               [Well]                      [[], []]      4\n",
       "3        [0]          [Excellent]                      [[], []]      4\n",
       "4        [0]                 [OK]                      [[], []]      2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app/1.A/raw/App_graphs_A.csv\",index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e7fdaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 271\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 11.009 \tMax: 116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>node_features</th>\n",
       "      <th>edges</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[play, service, ., Nice]</td>\n",
       "      <td>[[0, 2, 1, 3, 1, 0], [3, 1, 0, 0, 2, 1]]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[Wonderful, Excellence]</td>\n",
       "      <td>[[1, 0], [0, 1]]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[App, Nice]</td>\n",
       "      <td>[[0, 1], [1, 0]]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[from, speaking, 👎, f, any, my, much, ., and, ...</td>\n",
       "      <td>[[21, 0, 7, 18, 8, 26, 8, 7, 27, 11, 4, 20, 12...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[Jayesh, 6, bharwad, Saru]</td>\n",
       "      <td>[[3, 2, 1, 0, 2, 3], [2, 0, 3, 2, 3, 1]]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               nodes  \\\n",
       "0                                       [0, 1, 2, 3]   \n",
       "1                                             [0, 1]   \n",
       "2                                             [0, 1]   \n",
       "3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "4                                       [0, 1, 2, 3]   \n",
       "\n",
       "                                       node_features  \\\n",
       "0                           [play, service, ., Nice]   \n",
       "1                            [Wonderful, Excellence]   \n",
       "2                                        [App, Nice]   \n",
       "3  [from, speaking, 👎, f, any, my, much, ., and, ...   \n",
       "4                         [Jayesh, 6, bharwad, Saru]   \n",
       "\n",
       "                                               edges  label  \n",
       "0           [[0, 2, 1, 3, 1, 0], [3, 1, 0, 0, 2, 1]]      4  \n",
       "1                                   [[1, 0], [0, 1]]      4  \n",
       "2                                   [[0, 1], [1, 0]]      4  \n",
       "3  [[21, 0, 7, 18, 8, 26, 8, 7, 27, 11, 4, 20, 12...      0  \n",
       "4           [[3, 2, 1, 0, 2, 3], [2, 0, 3, 2, 3, 1]]      4  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.A test\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app/1.A/raw/App_graphs_A_test.csv\",index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0730a6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 417\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 10.955 \tMax: 130\n",
      "Vocabulary size ( nltk ): 271\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 11.009 \tMax: 116\n"
     ]
    }
   ],
   "source": [
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app/1.A2/raw/App_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app/1.A2/raw/App_graphs_A2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ef00940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 417\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 10.955 \tMax: 130\n",
      "Vocabulary size ( nltk ): 271\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 11.009 \tMax: 116\n"
     ]
    }
   ],
   "source": [
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_app/1.C/raw/App_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_app/1.C/raw/App_graphs_C_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f35c293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 417\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 10.955 \tMax: 130\n",
      "Vocabulary size ( nltk ): 271\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 11.009 \tMax: 116\n"
     ]
    }
   ],
   "source": [
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app/1.C2/raw/App_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app/1.C2/raw/App_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0935f7",
   "metadata": {},
   "source": [
    "#### Preprocesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24b5a558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonder ful fine'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/app_reviews/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/app_reviews/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a85a1573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6971, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b8fe321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.169559604074021 \tMax: 72\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.186497326203209 \tMax: 67\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.169559604074021 \tMax: 72\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.186497326203209 \tMax: 67\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app_prep/1.A/raw/App_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app_prep/1.A/raw/App_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app_prep/1.A2/raw/App_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app_prep/1.A2/raw/App_graphs_A2_test.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e56f8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.169559604074021 \tMax: 72\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.186497326203209 \tMax: 67\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.169559604074021 \tMax: 72\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 1 \tAverage: 6.186497326203209 \tMax: 67\n"
     ]
    }
   ],
   "source": [
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_app_prep/1.C/raw/App_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_app_prep/1.C/raw/App_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app_prep/1.C2/raw/App_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_app_prep/1.C2/raw/App_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990f0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "def clean_all_corpus(datasets):\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        print (\"\\n--------------------------\\nRunning data split for\", dataset)\n",
    "        df_train, df_test = dataset_to_dataframe(dataset)\n",
    "        \n",
    "        clean_corpus(dataset, df_train, mode=\"training\", num=7000, preprocess=\"raw\")\n",
    "        clean_corpus(dataset, df_test, mode=\"test\", num=3000, preprocess=\"raw\")\n",
    "\n",
    "        clean_corpus(dataset, df_train, mode=\"training\", num=7000, preprocess=\"soft\")\n",
    "        clean_corpus(dataset, df_test, mode=\"test\", num=3000, preprocess=\"soft\")\n",
    "\n",
    "        clean_corpus(dataset, df_train, mode=\"training\", num=7000, preprocess=\"tlgcn\")\n",
    "        clean_corpus(dataset, df_test, mode=\"test\", num=3000, preprocess=\"tlgcn\")\n",
    "\n",
    "        clean_corpus(dataset, df_train, mode=\"training\", num=7000, preprocess=\"custom\")\n",
    "        clean_corpus(dataset, df_test, mode=\"test\", num=3000, preprocess=\"custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f045f73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "Running data split for imdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/mbugueno/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 236.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "\n",
      "--------------------------\n",
      "Running data split for dbpedia_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 5.22kB [00:00, 867kB/s]                                                                                                                       \n",
      "Downloading metadata: 2.47kB [00:00, 592kB/s]                                                                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset dbpedia_14/dbpedia_14 (download: 65.18 MiB, generated: 191.44 MiB, post-processed: Unknown size, total: 256.62 MiB) to /home/mbugueno/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 68.3M/68.3M [00:03<00:00, 19.7MB/s]\n",
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dbpedia_14 downloaded and prepared to /home/mbugueno/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 438.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "7000 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "3000 / 3000 samples have been generated.\n",
      "\n",
      "--------------------------\n",
      "Running data split for hyperpartisan_news_detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.96kB [00:00, 797kB/s]                                                                                                                       \n",
      "Downloading metadata: 5.04kB [00:00, 619kB/s]                                                                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset hyperpartisan_news_detection/byarticle (download: 976.91 KiB, generated: 2.67 MiB, post-processed: Unknown size, total: 3.63 MiB) to /home/mbugueno/.cache/huggingface/datasets/hyperpartisan_news_detection/byarticle/1.0.0/7f4215b0474950ddf516e806400ab81d098b3da3b3a919a13cd1a4cf2c677012...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                                                                        | 0.00/972k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 972k/972k [00:00<00:00, 5.88MB/s]\u001b[A\n",
      "Downloading data files:  50%|███████████████████████████████████████████████████████▌                                                       | 1/2 [00:01<00:01,  1.45s/it]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.5k/28.5k [00:00<00:00, 6.54MB/s]\u001b[A\n",
      "Downloading data files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.27s/it]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 60.52it/s]\n",
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset hyperpartisan_news_detection downloaded and prepared to /home/mbugueno/.cache/huggingface/datasets/hyperpartisan_news_detection/byarticle/1.0.0/7f4215b0474950ddf516e806400ab81d098b3da3b3a919a13cd1a4cf2c677012. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 558.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "516 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "129 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "516 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "129 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "516 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "129 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "516 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "129 / 3000 samples have been generated.\n",
      "\n",
      "--------------------------\n",
      "Running data split for bbc\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data_Graphs/BBC/Raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m datasets\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbpedia_14\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperpartisan_news_detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclean_all_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mclean_all_corpus\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning data split for\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset)\n\u001b[0;32m----> 7\u001b[0m     df_train, df_test \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     clean_corpus(dataset, df_train, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7000\u001b[39m, preprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     clean_corpus(dataset, df_test, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, preprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/graph-based_TR/loader_TextLevel.py:116\u001b[0m, in \u001b[0;36mdataset_to_dataframe\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    113\u001b[0m         df_temp\u001b[38;5;241m=\u001b[39mfrom_dataset2df(dataset, my_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], columns)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#para bbc manually set dataframe \u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     df_temp\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Data_Graphs/BBC/Raw.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     df_temp\u001b[38;5;241m=\u001b[39mdf_temp\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m#read from path and split\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch111/lib/python3.8/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data_Graphs/BBC/Raw.csv'"
     ]
    }
   ],
   "source": [
    "datasets=[\"imdb\", \"dbpedia_14\", \"hyperpartisan_news_detection\", \"bbc\"]\n",
    "clean_all_corpus(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1337422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "Running data split for bbc\n",
      "Processing training set...\n",
      "1780 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "445 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "1780 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "445 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "1780 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "445 / 3000 samples have been generated.\n",
      "Processing training set...\n",
      "1780 / 7000 samples have been generated.\n",
      "Processing testing set...\n",
      "445 / 3000 samples have been generated.\n"
     ]
    }
   ],
   "source": [
    "datasets=[\"bbc\"]\n",
    "clean_all_corpus(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ca685",
   "metadata": {},
   "source": [
    "### DBpedia_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915c4e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Naucleopsis oblongifolia is a species of plant in the Moraceae family . It is endemic to Brazil .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/dbpedia_14/raw/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/dbpedia_14/raw/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f287e214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 1407\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 39.409 \tMax: 159\n",
      "Vocabulary size ( nltk ): 995\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 7 \tAverage: 39.192 \tMax: 127\n",
      "Vocabulary size ( nltk ): 1407\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 39.409 \tMax: 159\n",
      "Vocabulary size ( nltk ): 995\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 7 \tAverage: 39.192 \tMax: 127\n",
      "Vocabulary size ( nltk ): 1407\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 39.409 \tMax: 159\n",
      "Vocabulary size ( nltk ): 995\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 7 \tAverage: 39.192 \tMax: 127\n",
      "Vocabulary size ( nltk ): 1407\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 39.409 \tMax: 159\n",
      "Vocabulary size ( nltk ): 995\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 7 \tAverage: 39.192 \tMax: 127\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db/1.A/raw/DB_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db/1.A/raw/DB_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db/1.A2/raw/DB_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db/1.A2/raw/DB_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_db/1.C/raw/DB_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_db/1.C/raw/DB_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db/1.C2/raw/DB_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db/1.C2/raw/DB_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d3ac9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naucleopsis oblongifolia species plant moraceae family endemic brazil'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/dbpedia_14/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/dbpedia_14/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06ea60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 2 \tAverage: 26.325714285714287 \tMax: 124\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 26.165 \tMax: 101\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 2 \tAverage: 26.325714285714287 \tMax: 124\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 26.165 \tMax: 101\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 2 \tAverage: 26.325714285714287 \tMax: 124\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 26.165 \tMax: 101\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 2 \tAverage: 26.325714285714287 \tMax: 124\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 3 \tAverage: 26.165 \tMax: 101\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db_prep/1.A/raw/DB_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db_prep/1.A/raw/DB_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db_prep/1.A2/raw/DB_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db_prep/1.A2/raw/DB_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_db_prep/1.C/raw/DB_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_db_prep/1.C/raw/DB_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db_prep/1.C2/raw/DB_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_db_prep/1.C2/raw/DB_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4330406",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85cca8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"did n't know anything about the film or that it was based on a best selling book.Tried guessing from the opening scenes what it would be about , best guess , buried treasure and a death list.That lasted about 15 minutes when i got the sneaky suspicion that the film was crap.I 'll not bore you with how bad the plot and acting were but anyone who gave it more than two stars must work for the film makers.I watched until the hero jumped into his corvette to rush to the hospital.He had his on his suit , shirt and tie arrived at the hospital in jeans and a t/shirt.Could n't even get the continuity right . I got the Christian theme , hard to miss it .\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/imdb/raw/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/imdb/raw/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22bb3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 146\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 11 \tAverage: 154.13057142857141 \tMax: 692\n",
      "Vocabulary size ( nltk ): 129\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 150.429 \tMax: 640\n",
      "Vocabulary size ( nltk ): 146\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 11 \tAverage: 154.13057142857141 \tMax: 692\n",
      "Vocabulary size ( nltk ): 129\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 150.429 \tMax: 640\n",
      "Vocabulary size ( nltk ): 146\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 11 \tAverage: 154.13057142857141 \tMax: 692\n",
      "Vocabulary size ( nltk ): 129\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 150.429 \tMax: 640\n",
      "Vocabulary size ( nltk ): 146\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 11 \tAverage: 154.13057142857141 \tMax: 692\n",
      "Vocabulary size ( nltk ): 129\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 150.429 \tMax: 640\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb/1.A/raw/IMDB_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb/1.A/raw/IMDB_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb/1.A2/raw/IMDB_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb/1.A2/raw/IMDB_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb/1.C/raw/IMDB_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb/1.C/raw/IMDB_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb/1.C2/raw/IMDB_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb/1.C2/raw/IMDB_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9295a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'know anything film based best selling book tried guessing opening scenes would best guess buried treasure death list lasted 15 minutes got sneaky suspicion film crap bore bad plot acting anyone gave two stars must work film makers watched hero jumped corvette rush hospital suit shirt tie arrived hospital jeans shirt even get continuity right got christian theme hard miss'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/imdb/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/imdb/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "451f2a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 100.99114285714286 \tMax: 551\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 97.87833333333333 \tMax: 518\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 100.99114285714286 \tMax: 551\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 97.87833333333333 \tMax: 518\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 100.99114285714286 \tMax: 551\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 97.87833333333333 \tMax: 518\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 100.99114285714286 \tMax: 551\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 4 \tAverage: 97.87833333333333 \tMax: 518\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb_prep/1.A/raw/IMDB_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb_prep/1.A/raw/IMDB_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb_prep/1.A2/raw/IMDB_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb_prep/1.A2/raw/IMDB_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb_prep/1.C/raw/IMDB_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb_prep/1.C/raw/IMDB_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb_prep/1.C2/raw/IMDB_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_imdb_prep/1.C2/raw/IMDB_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ac2cf",
   "metadata": {},
   "source": [
    "### hyperpartisan_news_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae0981e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< p > CHARLOTTESVILLE , Va. - The reporter of a botched Rolling Stone article about a brutal gang rape at the University of Virginia willfully ignored facts and statements that disproved her preconceived storyline about the school & # 8217 ; s callousness toward sexual-assault victims , an attorney for the former associate dean of students said in his closing arguments Tuesday. < /p > < p > University administrator Nicole Eramo is seeking $ 7.5 million from the magazine over its portrayal of her in the 2014 story by Sabrina Rubin Erdely about the alleged sexual assault of a woman identified only as & # 8220 ; Jackie. & # 8221 ; Eramo claims she was unfairly portrayed in the article as trying to sweep Jackie & # 8217 ; s sexual assault under the rug in order to protect the university. < /p > < p > Attorney Tom Clare argued Tuesday that Erdely set out from the beginning to tell a story of & # 8220 ; institutional indifference , & # 8221 ; brushed off statements from her sources that didn & # 8217 ; t fit that narrative and pushed her own views about the administration onto the vulnerable women she was interviewing. < /p > < p > & # 8220 ; Once they decided what the article was going to be about , it didn & # 8217 ; t matter what the facts were , & # 8221 ; Clare said. < /p > < p > The story about Jackie & # 8217 ; s rape set off a firestorm at the University of Virginia and in schools nationwide and prompted police to launch an investigation into the alleged assault . Eramo received hundreds of angry letters and emails and faced protesters outside her office . The story crumbled after other news outlets began asking questions and police found no evidence to back up Jackie & # 8217 ; s claims . The article was officially retracted in April 2015. < /p > < p > Eramo must prove that Rolling Stone statements about her made her appear & # 8220 ; odious , infamous or ridiculous & # 8221 ; and that the magazine acted with & # 8220 ; actual malice , & # 8221 ; meaning it knew that what it was writing about her was false or should have known it was false. < /p > < p > A lawyer for Rolling Stone , Scott Sexton , said in closing statements there is no evidence the magazine knew a botched story about a gang rape at the University of Virginia was false before publishing it. < /p > < p > While the women Erdely interviewed - including Jackie - told her that Eramo was their fiercest advocate , Clare argued that Erdely was so invested in her preconceived storyline that she was & # 8220 ; blind to the facts. & # 8221 ; He argued that Erdely purposely set out to make Eramo the & # 8220 ; villain & # 8221 ; because she knew she was an & # 8220 ; easy target & # 8221 ; and couldn & # 8217 ; t speak publicly about Jackie & # 8217 ; s case due to federal privacy laws. < /p > < p > & # 8220 ; It & # 8217 ; s reckless , it & # 8217 ; s cavalier and it & # 8217 ; s intentional , & # 8221 ; Clare said. < /p > < p > Over the course of the more than two-week trial , the 10 jurors have watched 11 hours of video testimony , heard from a dozen live witnesses and have examined nearly 300 exhibits . Seven jurors will deliberate and three will be named as alternates. < /p > < p > & # 169 ; 2016 The Associated Press . All Rights Reserved . This material may not be published , broadcast , rewritten , or redistributed. < /p >'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/hyperpartisan_news_detection/raw/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/hyperpartisan_news_detection/raw/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08bb7b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 92\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 21 \tAverage: 301.8759689922481 \tMax: 1402\n",
      "Vocabulary size ( nltk ): 88\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 55 \tAverage: 309.0387596899225 \tMax: 950\n",
      "Vocabulary size ( nltk ): 92\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 21 \tAverage: 301.8759689922481 \tMax: 1402\n",
      "Vocabulary size ( nltk ): 88\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 55 \tAverage: 309.0387596899225 \tMax: 950\n",
      "Vocabulary size ( nltk ): 92\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 21 \tAverage: 301.8759689922481 \tMax: 1402\n",
      "Vocabulary size ( nltk ): 88\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 55 \tAverage: 309.0387596899225 \tMax: 950\n",
      "Vocabulary size ( nltk ): 92\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 21 \tAverage: 301.8759689922481 \tMax: 1402\n",
      "Vocabulary size ( nltk ): 88\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 55 \tAverage: 309.0387596899225 \tMax: 950\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection/1.A/raw/Hyper_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection/1.A/raw/Hyper_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection/1.A2/raw/Hyper_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection/1.A2/raw/Hyper_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection/1.C/raw/Hyper_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection/1.C/raw/Hyper_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection/1.C2/raw/Hyper_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection/1.C2/raw/Hyper_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bbc17d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p charlottesville va reporter botched rolling stone article brutal gang rape university virginia willfully ignored facts statements disproved preconceived storyline school 8217 callousness toward sexual assault victims attorney former associate dean students said closing arguments tuesday p p university administrator nicole eramo seeking 7 5 million magazine portrayal 2014 story sabrina rubin erdely alleged sexual assault woman identified 8220 jackie 8221 eramo claims unfairly portrayed article trying sweep jackie 8217 sexual assault rug order protect university p p attorney tom clare argued tuesday erdely set beginning tell story 8220 institutional indifference 8221 brushed statements sources 8217 fit narrative pushed views administration onto vulnerable women interviewing p p 8220 decided article going 8217 matter facts 8221 clare said p p story jackie 8217 rape set firestorm university virginia schools nationwide prompted police launch investigation alleged assault eramo received hundreds angry letters emails faced protesters outside office story crumbled news outlets began asking questions police found evidence back jackie 8217 claims article officially retracted april 2015 p p eramo must prove rolling stone statements made appear 8220 odious infamous ridiculous 8221 magazine acted 8220 actual malice 8221 meaning knew writing false known false p p lawyer rolling stone scott sexton said closing statements evidence magazine knew botched story gang rape university virginia false publishing p p women erdely interviewed including jackie told eramo fiercest advocate clare argued erdely invested preconceived storyline 8220 blind facts 8221 argued erdely purposely set make eramo 8220 villain 8221 knew 8220 easy target 8221 8217 speak publicly jackie 8217 case due federal privacy laws p p 8220 8217 reckless 8217 cavalier 8217 intentional 8221 clare said p p course two week trial 10 jurors watched 11 hours video testimony heard dozen live witnesses examined nearly 300 exhibits seven jurors deliberate three named alternates p p 169 2016 associated press rights reserved material may published broadcast rewritten redistributed p'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/hyperpartisan_news_detection/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/hyperpartisan_news_detection/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac1e64bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 230.51162790697674 \tMax: 1240\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 28 \tAverage: 234.50387596899225 \tMax: 824\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 230.51162790697674 \tMax: 1240\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 28 \tAverage: 234.50387596899225 \tMax: 824\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 230.51162790697674 \tMax: 1240\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 28 \tAverage: 234.50387596899225 \tMax: 824\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 8 \tAverage: 230.51162790697674 \tMax: 1240\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 28 \tAverage: 234.50387596899225 \tMax: 824\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection_prep/1.A/raw/Hyper_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection_prep/1.A/raw/Hyper_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection_prep/1.A2/raw/Hyper_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection_prep/1.A2/raw/Hyper_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection_prep/1.C/raw/Hyper_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection_prep/1.C/raw/Hyper_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection_prep/1.C2/raw/Hyper_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_hyperpartisan_news_detection_prep/1.C2/raw/Hyper_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781960bc",
   "metadata": {},
   "source": [
    "### BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a57f400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Legendary music studio to close The New York music studio where John Lennon spent his final hours is to close next month . The Hit Factory , which opened 37 years ago , has played host to some of the biggest stars in music , including Paul Simon , Madonna and David Bowie . However , the rise in digital recording has affected business at the studio , which is relocating to smaller premises in Miami . Lennon recorded his final album Double Fantasy at the studio in 1979 . The studio was founded by Jerry Ragovoy in 1968 . In 1975 it was sold to fellow producer Edward Germano , who turned it into a 100,000 square foot studio with seven recording rooms and live mastering suites . His first client was Stevie Wonder , who recorded part of his classic album Songs In The Key Of Life there . Other well-known albums to be recorded or partially recorded at the studio include Bruce Springsteen 's Born In The USA , the Rolling Stones ' Emotional Rescue and Paul Simon 's Graceland . Michael Jackson , Billy Joel , Jay-Z and Beyonce are also among artists who have used the Hit Factory in the past , as well as 50 Cent who survived an attempt on his life as he left the premises in 2000 . The studio made history in 1994 when it won a record 41 Grammy nominations for songs recorded , mastered or mixed there , including the soundtrack to the Whitney Houston film The Bodyguard .\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/bbc/raw/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/bbc/raw/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0baa1ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 85\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 71 \tAverage: 215.72359550561796 \tMax: 1372\n",
      "Vocabulary size ( nltk ): 81\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 92 \tAverage: 225.58876404494382 \tMax: 1020\n",
      "Vocabulary size ( nltk ): 85\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 71 \tAverage: 215.72359550561796 \tMax: 1372\n",
      "Vocabulary size ( nltk ): 81\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 92 \tAverage: 225.58876404494382 \tMax: 1020\n",
      "Vocabulary size ( nltk ): 85\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 71 \tAverage: 215.72359550561796 \tMax: 1372\n",
      "Vocabulary size ( nltk ): 81\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 92 \tAverage: 225.58876404494382 \tMax: 1020\n",
      "Vocabulary size ( nltk ): 85\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 71 \tAverage: 215.72359550561796 \tMax: 1372\n",
      "Vocabulary size ( nltk ): 81\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 92 \tAverage: 225.58876404494382 \tMax: 1020\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc/1.A/raw/BBC_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc/1.A/raw/BBC_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc/1.A2/raw/BBC_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc/1.A2/raw/BBC_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc/1.C/raw/BBC_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc/1.C/raw/BBC_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc/1.C2/raw/BBC_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc/1.C2/raw/BBC_graphs_C2_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "850187de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'legendary music studio close new york music studio john lennon spent final hours close next month hit factory opened 37 years ago played host biggest stars music including paul simon madonna david bowie however rise digital recording affected business studio relocating smaller premises miami lennon recorded final album double fantasy studio 1979 studio founded jerry ragovoy 1968 1975 sold fellow producer edward germano turned 100 000 square foot studio seven recording rooms live mastering suites first client stevie wonder recorded part classic album songs key life well known albums recorded partially recorded studio include bruce springsteen born usa rolling stones emotional rescue paul simon graceland michael jackson billy joel jay z beyonce also among artists used hit factory past well 50 cent survived attempt life left premises 2000 studio made history 1994 record 41 grammy nominations songs recorded mastered mixed including soundtrack whitney houston film bodyguard'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"../Data_RBased/bbc/soft/source_processed.csv\")\n",
    "df_test=pd.read_csv(\"../Data_RBased/bbc/soft/source_processed_test.csv\")\n",
    "np.asarray(df_train['content'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfe765d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 42 \tAverage: 158.19044943820225 \tMax: 1149\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 64 \tAverage: 165.59101123595505 \tMax: 819\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 42 \tAverage: 158.19044943820225 \tMax: 1149\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 64 \tAverage: 165.59101123595505 \tMax: 819\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 42 \tAverage: 158.19044943820225 \tMax: 1149\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 64 \tAverage: 165.59101123595505 \tMax: 819\n",
      "Vocabulary size ( nltk ): 37\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 42 \tAverage: 158.19044943820225 \tMax: 1149\n",
      "Vocabulary size ( nltk ): 36\n",
      "\n",
      "Lenght of text (w.r.t. #tokens):\n",
      "Min: 64 \tAverage: 165.59101123595505 \tMax: 819\n"
     ]
    }
   ],
   "source": [
    "#1.A \n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc_prep/1.A/raw/BBC_graphs_A.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='forward', ws=1, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc_prep/1.A/raw/BBC_graphs_A_test.csv\",index=False)\n",
    "\n",
    "#1.A2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc_prep/1.A2/raw/BBC_graphs_A2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=False, mode='window', ws=2, directed=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc_prep/1.A2/raw/BBC_graphs_A2_test.csv\",index=False)\n",
    "\n",
    "#1.C\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc_prep/1.C/raw/BBC_graphs_C.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False, weighted=True, mode='forward', ws=1, directed=True)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc_prep/1.C/raw/BBC_graphs_C_test.csv\",index=False)\n",
    "\n",
    "#1.C2\n",
    "df=create_graph2Dataframe(df_train['content'], df_train['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc_prep/1.C2/raw/BBC_graphs_C2.csv\",index=False)\n",
    "\n",
    "df=create_graph2Dataframe(df_test['content'], df_test['label'], plot=False)\n",
    "df.to_csv(\"../Data_Graphs/data_bbc_prep/1.C2/raw/BBC_graphs_C2_test.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
